---
title: "Bayesian Analysis in Psychology"
output: 
  rmarkdown::html_vignette:
    toc: true
author: 
- Dominique Makowski
date: "`r Sys.Date()`"
tags: [r, psychology, neuroscience]
abstract: |
  Why use frequentist methods when you can use, in an even simpler way, the Bayesian framework? Throughout this tutorial, we will explore many of the analyses you might want to do with your data.
vignette: >
  %\VignetteIndexEntry{BayesianPsychology}
  %\VignetteDepends{dplyr}
  %\VignetteDepends{tidyr}
  %\VignetteDepends{ggplot2}
  %\VignetteDepends{plotly}
  \usepackage[utf8]{inputenc}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---



------

```{r, echo=F, message=FALSE, warning=FALSE}
library(knitr)
library(rstanarm)
library(emmeans)
library(dplyr)
library(tidyr)
library(ggplot2)
library(psycho)
options(mc.cores=1)
```


## The Bayesian Framework

### Why use the Bayesian Framework?

In short, because it's:

- Better
- Simpler
- Superior
- Preferable
- More appropriate
- More desirable
- More useful
- More valuable

##### **From Makowski et al. (*under review*):**

> Reasons to prefer this approach is reliability, better accuracy in noisy data, better estimation for small samples, less prone to type I error, the possibility of introducing prior knowledge into the analysis and, critically, results intuitiveness and their straightforward interpretation (Andrews & Baguley, 2013; Etz & Vandekerckhove, 2016; Kruschke, 2010; Kruschke, Aguinis, & Joo, 2012; Wagenmakers et al., 2018). Indeed, in the frequentist view, the effects are fixed (but unknown) and data are random, while the Bayesian inference calculates the probability of different effect values (called the **"posterior" distribution**) given the observed data. Bayesian’s uncertainty can be summarized, for example, by giving a range of values on the posterior distribution that includes 95% of the probability (the 95% *Credible Interval*). To illustrate the difference, the Bayesian framework allows to say "*given the observed data, the effect has 95% probability of falling within this range*", while the Frequentist less straightforward alternative would be "*there is a 95% probability that when computing a confidence interval from data of this sort, the effect falls within this range*". In general, the frequentist approach has been associated with the focus on null hypothesis testing, and the misuse of *p* values has been shown to critically contribute to the reproducibility crisis of psychological science (Chambers, Feredoes, Muthukumaraswamy, Suresh, & Etchells, 2014; Szucs & Ioannidis, 2016). There is a general agreement that the generalization of the Bayesian approach is a way of overcoming these issues (Benjamin et al., 2018; Etz & Vandekerckhove, 2016).

### What is the Bayesian Framework?

Once we agreed that the Bayesian framework is the right way to go, you might wonder what is the Bayesian framework. **What's all the fuss about?**

Omitting the maths behind it, let's just say that:

- The frequentist guy tries to estimate "the real effect". The "real" value of the correlation between X and Y. It returns a "point-estimate" (*i.e.*, a single value) of the "real" correlation (*e.g.*, r = 0.42), considering that the data is sampled at random from a "parent", usually normal distribution of data.
- **The Bayesian master assumes no such thing**. The data are what they are. Based on this observed data (and eventually from its expectations), the Bayesian sampling algorithm will return a probability distribution of the effect that is compatible with the observed data. For the correlation between X and Y, it will return a distribution that says "the most probable effect is 0.42, but this data is also compatible with correlations of 0.12 or 0.74".
- To characterize our effects, **no need of p values** or other mysterious indices. We simply describe the posterior distribution (*i.e.*, the distribution of the effect). We can present the median (better than the mean, as it actually means that the effect has 50% of chance of being higher and 50% of chance of being lower), the MAD (a median-based, robust equivalent of SD) and other stuff such as the 90% HDI, called here *credible interval*.


### The "Posterior" distribution

```{r message=FALSE, warning=FALSE, include=FALSE}
X <- psycho::standardize(psycho::affective$Concealing)
Y <- psycho::standardize(psycho::affective$Life_Satisfaction)
r <- cor.test(X, Y)$estimate
p <- cor.test(X, Y)$p.value
fit <- rstanarm::stan_glm(Y ~ X, seed=666, data=data.frame(Y,X))
values <- values(analyze(fit))
posterior <- values$effects$X$posterior
density <- density(posterior, n = length(posterior))
hdi <- hdi(posterior, 0.90)
mpe <- mpe(posterior)$MPE
```

Let's imagine two numeric variables, Y and X. The correlation between them is r = `r psycho::format_digit(r)` (p < .05). A Bayesian analysis would return the probability of distribution of this effect (the **posterior**), that we can characterize using several indices (centrality (**median** or mean), dispersion (SD or Median Absolute Deviation - **MAD**), etc.). Let's plot the posterior distribution of the possible correlation values that are compatible with our data.

```{r echo=FALSE, message=FALSE, warning=FALSE, fig.width=7, fig.height=4.5, fig.align='center', fig.cap="Posterior probability distribution of the correlation between X and Y"}
ggplot(data.frame(x = density$x, y = density$y), aes(x=x, y=y)) +
  xlab("\nPosterior Distribution of Correlation") + 
  ylab("Density") +
  annotate("rect", xmin = hdi$values$HDImin, xmax=hdi$values$HDImax, ymin = 0, ymax=round(max(density$y)), fill="#2196F3", alpha = .5) +
  geom_segment(aes(xend = x, yend = 0, colour = x), alpha=0.8) + 
  scale_color_gradientn(colours = c("#E91E63", "#E91E63", "#4CAF50", "#4CAF50"),
                         values=c(0, 0.4999, 0.5, 1),
                         guide=F,
                         limits = c(-1.5, 1.5)) +
  # r
  geom_vline(xintercept=r, color="#4CAF50") +
  annotate("segment", x = r+0.05, xend = r, y = 10, yend = 10, size=0.1, arrow=arrow(type="closed", length = unit(0.10, "inches")), color="#4CAF50") +
  annotate("text", x = r+0.055, y = 10, label = "Frequentist r Coefficient" , size=4 , fontface="bold", hjust = 0, color="#4CAF50") +
  # median
  geom_vline(xintercept=median(posterior)) +
  annotate("segment", x = median(posterior)+0.05, xend = r, y = 8, yend = 8, colour = "black", size=0.1, arrow=arrow(type="closed", length = unit(0.10, "inches"))) +
  annotate("text", x = median(posterior)+0.055, y = 8, label = "Posterior's Median" , size=4 , fontface="bold", hjust = 0) +
  # # mean
  # geom_vline(xintercept=mean(posterior), color="#2196F3") +
  # annotate("segment", x = mean(posterior)+0.03, xend = r, y = 6, yend = 6, size=0.1, arrow=arrow(type="closed", length = unit(0.10, "inches")), color="#2196F3") +
  # annotate("text", x = mean(posterior)+0.035, y = 6, label = "mean" , size=4 , fontface="bold", hjust = 0, color="#2196F3")
  annotate("segment", x = hdi$values$HDImin, xend = hdi$values$HDImax, y = 3, yend = 3, size=0.3, arrow=arrow(type="closed", ends="both", length = unit(0.10, "inches")), color="#2196F3") +
  annotate("text", x = -0.01, y = 3, label = "90% Credible Interval" , size=4 , fontface="bold", hjust = 0, color="#2196F3")
```

In this example (based on real data):

- The **median of the posterior distribution is really close to the *r* coefficient obtained through a "normal" correlation analysis**, appearing as a good point-estimate of the correlation. Moreover, and unlike the frequentist estimate, the median has an intuitive meaning (as it cuts the distribution in two equal parts): the "true" effect has 50% of chance of being higher and 50% of chance of being lower. 
- We can also compute the **90% *credible* interval**, which shows where the true effect can fall with a probability of 90% (better than 95% CI, see *Kruschke, 2015*). 
- Finally, we can also compute the **MPE**, *i.e.*, the maximum probability of effect, which is the probability that the effect is in the median's direction (*i.e.*, negative in this example): the area in red. It interesting to note that the probability that the effect is opposite ((100 - MPE) / 100, here (100-`r mpe`) / 100 = `r format_digit((100-mpe)/100)`) is relatively close to the actual **p value** obtained through frequentist correlation (p = `r format_digit(p)`).

**Now that you're familiar with posterior distributions, the core difference of the Bayesian framework, let's practice!**

## The affective Dataset

Let's start by taking a look at the dataset included within the `psycho` package.

```{r, echo=T, message=FALSE, warning=FALSE, results='hide'}
library(rstanarm)
library(dplyr)
library(ggplot2)
library(psycho)

df <- psycho::affective
summary(df)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
summary(df)
```

The data include **5 continuous variables** (age, life satisfaction and 3 affective styles) and **3 factors** (sex, salary and season of birth).



## Simple Regression (*Correlation*)

Let's start with something simple : a **correlation**. To simplify, a (Pearson's) correlation is pretty much nothing more than a simple linear regression (with standardized variables). Let's see if there's a linear relationship between **Life Satisfaction** and the tendency of **Tolerating** our emotions using a Bayesian linear regression model.

### Model Exploration

```{r, message=FALSE, results="hide"}
# Let's fit our model
fit <- rstanarm::stan_glm(Life_Satisfaction ~ Tolerating, data=df)
```




Let's check the results: 
```{r, message=FALSE, results="hide"}
# Format the results using analyze()
results <- psycho::analyze(fit)

# We can extract a formatted summary table
summary(results, round = 2)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::kable(summary(results, round = 2))
```

For each parameter of the model, the summary shows:

- the **median** of the posterior distribution of the parameter (can be used as a point estimate, similar to the beta of frequentist models)
- the **Median Absolute Deviation (MAD)**, a robust measure of dispertion (could be seen as a robust version of SD)
- the **Credible Interval** (by default, the 90\% CI; see Kruschke, 2018), representing the range of parameter values possibility
- the **Maximum Probability of Effect (MPE)**, an index of effect existence (the probability that the effect is different from 0 in the median's direction)

It also returns the (unadjusted) R2 (which represents the *percentage of variance of the outcome explained by the model*). In the Bayesian framework, the R2 is also estimated with probabilities. As such, characteristics of its posterior distribution are returned.



We can also print a formatted version:
```{r echo=TRUE, message=FALSE, warning=FALSE}
print(results)
```

Note that the  `print()` returns also additional info, such as the 100\% CI of the R2 and, for each parameter, the limits of the range defined by the MPE. 



### Interpretation


For now, omit the part dedicated to priors. We'll see it in the next chapters. Let's rather interpret the part related to effects.

> Full Bayesian mixed linear models are fitted using the rstanarm R wrapper for the stan probabilistic language (Gabry & Goodrich, 2016). Bayesian inference was done using Markov Chain Monte Carlo (MCMC) sampling. The prior distributions of all effects were set as weakly informative (mean = 0, SD = `r psycho::format_digit(results$values$effects$Tolerating$prior$adjusted_scale)`), meaning that we did not expect effects different from null in any particular direction. For each model and each coefficient, we will present several characteristics of the posterior distribution, such as its median (a robust estimate comparable to the beta from frequentist linear models), MAD (median absolute deviation, a robust equivalent of standard deviation) and the 90% credible interval. Instead of the *p value* as an index of effect existence, we also computed the maximum probability of effect (MPE), *i.e.*, the maximum probability that the effect is different from 0 in the median’s direction. For our analyses, we will consider an effect as inconsistent (*i.e.*, not probable enough) if its MPE is lower than 90% (however, **beware not to fall in a *p* value-like obsession**).


The current model explains about `r psycho::format_digit(results$values$effects$R2$median*100)`% of life satisfaction variance. Within this model, a positive linear relationship between life satisfaction and tolerating exists with high probability (Median = `r psycho::format_digit(results$values$effects$Tolerating$median)`, MAD = `r psycho::format_digit(results$values$effects$Tolerating$mad)`, 90% CI [`r paste(psycho::format_digit(results$values$effects$Tolerating$CI_values), collapse = ', ')`], MPE = `r psycho::format_digit(results$values$effects$Tolerating$MPE)`%).

### Model Visualization

To visualize the model, the most neat way is to extract a "reference grid" (*i.e.*, a theorethical dataframe with balanced data).

```{r echo=T, message=FALSE, warning=FALSE}
refgrid <- df %>% 
  select(Tolerating) %>% 
  psycho::refdata(length.out=10)

predicted <- psycho::get_predicted(fit, newdata=refgrid)
```
```{r echo=T, message=FALSE, warning=FALSE, results='hide'}
predicted
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(predicted)
```

Our refgrid is made of equally spaced (balanced) predictor values. It also include the median of the posterior prediction, as well as 90% credible intervals. Now, we can plot it as follows:

```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='markup', fig.align='center', comment=NA}
ggplot(predicted, aes(x=Tolerating, y=Life_Satisfaction_Median)) +
  geom_line() +
  geom_ribbon(aes(ymin=Life_Satisfaction_CI_5, 
                  ymax=Life_Satisfaction_CI_95), 
              alpha=0.1)
```


## Regression with Categorical Predictor (*ANOVA*)

When the predictor is categorical, simplifying the model is called running an ANOVA. Let's do it by answering the following question: does the level of **life satisfaction** depend on the salary? 

### Model Exploration

```{r, message=FALSE, results="hide"}
# Let's fit our model
fit <- rstanarm::stan_glm(Life_Satisfaction ~ Salary, data=df)
```
Let's check the results: 
```{r, message=FALSE, warning=FALSE}
# Format the results using analyze()
results <- psycho::analyze(fit)

# We can extract a formatted summary table
print(results)
```

### Post-hoc / Contrasts / Comparisons

What interest us is the pairwise comparison between the groups. The `get_contrasts` function computes the estimated marginal means (least-squares means), *i.e.*, the means of each group estimated by the model, as well as the contrasts.

```{r, message=FALSE, results="hide"}
contrasts <- psycho::get_contrasts(fit, "Salary")
```

We can see the estimated means like that:
```{r echo=T, message=FALSE, warning=FALSE, results='hide'}
contrasts$means
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(contrasts$means, digits=2)
```


And the contrasts comparisons like that:
```{r echo=T, message=FALSE, warning=FALSE, results='hide'}
contrasts$contrasts
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(contrasts$contrasts, digits=2)
```

As we can see, the only probable difference (MPE > 90%) is between **Salary <1000** and **Salary 2000+**.

### Model Visualization

```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='markup', fig.align='center', comment=NA}
ggplot(contrasts$means, aes(x=Level, y=Median, group=1)) +
  geom_line() +
  geom_pointrange(aes(ymin=CI_lower, ymax=CI_higher)) +
  ylab("Life Satisfaction") +
  xlab("Salary")
```


## Logistic Regressions

Let's see if we can **predict the sex** with the tendency to flexibly *adjust* our emotional reactions. As the Sex is a binary factor (with two modalities), we have to fit a logistic model.

### Model Exploration

```{r, message=FALSE, results="hide"}
# Let's fit our model
fit <- rstanarm::stan_glm(Sex ~ Adjusting, data=df, family = "binomial")
```



First, let's check our model: 
```{r, message=FALSE, results="hide"}
# Format the results using analyze()
results <- psycho::analyze(fit)

# We can extract a formatted summary table
summary(results, round = 2)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(summary(results, round = 2))
```

It appears that the link between adjusting and the sex is highly probable (MPE > 90%). But in what direction? To know that, we have to find out what is the intercept (the reference level).


```{r echo=FALSE, message=FALSE, warning=FALSE}
levels(df$Sex)
```
As **female** is the first level, it means that it is the intercept. Based on our model, an increase of 1 on the scale of **adjusting** will increase the probability (expressed in log odds ratios) of being a **male**.

### Model Visualization

To visualize this type of model, we have to derive a reference grid.

```{r echo=T, message=FALSE, warning=FALSE}
refgrid <- df %>% 
  select(Adjusting) %>% 
  psycho::refdata(length.out=10)
  
predicted <- psycho::get_predicted(fit, newdata=refgrid)
```

Note that `get_predicted` automatically transformed log odds ratios (the values in which the model is expressed) to probabilities, easier to apprehend.

```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='markup', fig.align='center', comment=NA}
ggplot(predicted, aes(x=Adjusting, y=Sex_Median)) +
  geom_line() +
  geom_ribbon(aes(ymin=Sex_CI_5, 
                  ymax=Sex_CI_95), 
              alpha=0.1) +
  ylab("Probability of being a male")
```

We can nicely see the non-linear relationship between adjusting and the probability of being a male.

## Multiple Regressions and MANOVAs / ANCOVAs

Let's create models a bit more complex, mixing factors with numeric predictors, to see if the **life satisfaction** is related to the tendency to suppress, **conceal** the emotional reactions, and does this relationship depends on the **sex**.

### Model Exploration

```{r, message=FALSE, results="hide"}
# Let's fit our model
fit <- rstanarm::stan_glm(Life_Satisfaction ~ Concealing * Sex, data=df)
```



Let's check our model: 
```{r, message=FALSE, results="hide"}
# Format the results using analyze()
results <- psycho::analyze(fit)

# We can extract a formatted summary table
summary(results, round = 2)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(summary(results, round = 2))
```

Again, it is important to notice that the intercept (the baseline) corresponds here to **Concealing = 0** and **Sex = F**. As we can see next, there is, with high probability, a negative linear relationship between concealing (*for females only*) and life satisfaction. Also, at the (theorethical) intercept (when concealing = 0), the males have a lower life satisfaction. Finally, the interaction is also probable. This means that when the participant is a male, the relationship between concealing and life satisfaction is significantly different (increased by 0.17. In other words, we could say that the relationship is of -0.10+0.17=0.07 in men).

### Model Visualization

How to represent this type of models? Again, we have to generate a reference grid.

```{r echo=T, message=FALSE, warning=FALSE, results="hide"}
refgrid <- df %>% 
  select(Concealing, Sex) %>% 
  psycho::refdata(length.out=10)

predicted <- psycho::get_predicted(fit, newdata=refgrid)
predicted
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(predicted)
```

As we can see, the reference grid is balanced in terms of factors and numeric predictors. Now, to plot this becomes very easy!


```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='markup', fig.align='center', comment=NA}
ggplot(predicted, aes(x=Concealing, y=Life_Satisfaction_Median, fill=Sex)) +
  geom_line(aes(colour=Sex)) +
  geom_ribbon(aes(fill=Sex,
                  ymin=Life_Satisfaction_CI_5, 
                  ymax=Life_Satisfaction_CI_95), 
              alpha=0.1) +
  ylab("Life Satisfaction")
```

We can see that the error for the males is larger, due to less observations. 



## Mixed Models


### Why use mixed-models?

- **From Makowski et al. (*under review*):**

> The Mixed modelling framework allows estimated effects to vary by group at lower levels while estimating population-level effects through the specification of fixed (explanatory variables) and random (variance components) effects. Outperforming traditional procedures such as repeated measures ANOVA (Kristensen & Hansen, 2004), these models are particularly suited to cases in which experimental stimuli are heterogeneous (e.g., images) as the item-related variance, in addition to the variance induced by participants, can be accounted for (Baayen, Davidson, & Bates, 2008; Magezi, 2015). Moreover, mixed models can handle unbalanced data, nested designs, crossed random effects and missing data.

As for how to run this type of analyses, it is quite easy. Indeed, all what has been said previously remains the same for mixed models. Except that there are random effects (specified by putting `+ (1|random_term)` in the formula). For example, we might want to consider the **salary** as a random effect (to "**adjust**" (*so to speak*) for the fact that the data is structured in two groups). Let's explore the relationship between the tendency to **conceal** emotions and **age** (*adjusted* for **salary**).

### Model Exploration

```{r eval=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
# Let's fit our model (it takes more time)
fit <- rstanarm::stan_lmer(Concealing ~ Age + (1|Salary), data=df)
```
```{r message=FALSE, warning=FALSE, include=FALSE, results="hide"}
# Let's fit our model (it takes more time)
fit <- rstanarm::stan_lmer(Concealing ~ Age + (1|Salary), data=df, iter=500, chains=2, seed=666)
```

Let's check our model: 
```{r, message=FALSE, results="hide"}
# Format the results using analyze()
results <- psycho::analyze(fit)

# We can extract a formatted summary table
summary(results, round = 2)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(summary(results, round = 2))
```

As we can see, the linear relationship has only a moderate probability of being different from 0.

### Model Visualization


```{r echo=T, message=FALSE, warning=FALSE}
refgrid <- df %>% 
  select(Age) %>% 
  psycho::refdata(length.out=10)

# We name the predicted dataframe by adding '_linear' to keep it for further comparison (see next part)
predicted_linear <- psycho::get_predicted(fit, newdata=refgrid)
```
```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='markup', fig.align='center', comment=NA}
ggplot(predicted_linear, aes(x=Age, y=Concealing_Median)) +
  geom_line() +
  geom_ribbon(aes(ymin=Concealing_CI_5, 
                  ymax=Concealing_CI_95), 
              alpha=0.1)
```

## Polynomial Transformations

Relationships in the real world are often non-linear. For example, based on the previous relationship between **concealing** and **age**, we could try modelling a polynomial (second order) transformation to the predictor.

### Model Exploration

```{r, message=FALSE, results="hide", warning=FALSE, eval=FALSE}
# Let's fit our model (it takes more time)
fit <- rstanarm::stan_lmer(Concealing ~ poly(Age, 2, raw=TRUE) + (1|Salary), data=df)
```
```{r message=FALSE, warning=FALSE, include=FALSE, results="hide"}
# Let's fit our model (it takes more time)
fit <- rstanarm::stan_lmer(Concealing ~ poly(Age, 2, raw=TRUE) + (1|Salary), data=df, iter=500, chains=2)
```

Let's check our model: 
```{r, message=FALSE, results="hide"}
# Format the results using analyze()
results <- psycho::analyze(fit)

# We can extract a formatted summary table
summary(results, round = 2)
```
```{r echo=FALSE, message=FALSE, warning=FALSE}
kable(summary(results, round = 2))
```

As we can see, both the linear relationship and the second order curvature are highly probable. However, when setting `raw=TRUE` in the formula, the coefficients become unintepretable. So let's visualize them.

### Model Visualization

The model visualization routine is similar to the previous ones.

```{r echo=T, message=FALSE, warning=FALSE}
refgrid <- df %>% 
  select(Age) %>% 
  psycho::refdata(length.out=20)

predicted_poly <- psycho::get_predicted(fit, newdata=refgrid)
```
```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='markup', fig.align='center', comment=NA}
ggplot(predicted_poly, aes(x=Age, y=Concealing_Median)) +
  geom_line() +
  geom_ribbon(aes(ymin=Concealing_CI_5, 
                  ymax=Concealing_CI_95), 
              alpha=0.1)
```

As we can see, adding the polynomial degree changes the relationship. Since the model is here very simple, we can add on the plot the actual points (however, they do not take into account the random effects and such), as well as plot the two models. Also, let's make it "dynamic" using `plotly`.

```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='markup', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
p <- ggplot() +
  # Linear model
  geom_line(data=predicted_linear, 
            aes(x=Age, y=Concealing_Median),
            colour="blue",
            size=1) +
  geom_ribbon(data=predicted_linear, 
              aes(x=Age,
                  ymin=Concealing_CI_5,
                  ymax=Concealing_CI_95), 
              alpha=0.1,
              fill="blue") +
  # Polynormial Model
  geom_line(data=predicted_poly, 
            aes(x=Age, y=Concealing_Median),
            colour="red",
            size=1) +
  geom_ribbon(data=predicted_poly, 
              aes(x=Age,
                  ymin=Concealing_CI_5, 
                  ymax=Concealing_CI_95), 
              fill="red",
              alpha=0.1) +
  # Actual data
  geom_point(data=df, aes(x=Age, y=Concealing))

library(plotly) # To create interactive plots
ggplotly(p) # To transform a ggplot into an interactive plot
```


**It's good to take a few steps back and look at the bigger picture :)**

<!-- But which one, between these two models, is "objectively" better? -->

<!-- ## Model Comparison and Selection -->

<!-- It is often interesting to know which model better fits the data. The frequentist approach provides several indices of goodness of fit (AIC, BIC, ...), for which the Bayesian framework has equivalents. These are obtained through **Leave-one-out cross-validation (LOO)** and are called **ELPD** (*expected log predictive density*) and **LOOIC** (*LOO information criterion*). The best model is the one with  the largest ELPD (smallest LOOIC). See the [official rstanarm documentation](https://CRAN.R-project.org/package=rstanarm/vignettes/rstanarm.html#step-3-criticize-the-model) as well as the [loo package](https://CRAN.R-project.org/package=loo/vignettes/loo2-example.html) for details. -->

<!-- ```{r message=FALSE, warning=FALSE, include=FALSE, results="hide"} -->
<!-- # Fit the models -->
<!-- fit_intercept_only <- rstanarm::stan_glm(Concealing ~ 1, data=df, cores=1, chains=1, seed=666) -->
<!-- fit_linear <- rstanarm::stan_glm(Concealing ~ Age, data=df, cores=1, chains=1, seed=666) -->
<!-- fit_poly <- rstanarm::stan_glm(Concealing ~ poly(Age, 2, raw=TRUE), data=df, cores=1, chains=1, seed=666) -->

<!-- # Compute the LOO validation -->
<!-- loo_intercept_only <- rstanarm::loo(fit_intercept_only, cores=1) -->
<!-- loo_linear <- rstanarm::loo(fit_linear, cores=1) -->
<!-- loo_poly <- rstanarm::loo(fit_poly, cores=1) -->

<!-- # Compare the models -->
<!-- comparison <- rstanarm::compare_models(loo_intercept_only, -->
<!--                                        loo_linear, -->
<!--                                        loo_poly) -->
<!-- print(comparison) -->
<!-- ``` -->
<!-- ```{r, message=FALSE, results="hide", eval=FALSE, warning=FALSE} -->
<!-- # Fit the models -->
<!-- fit_intercept_only <- rstanarm::stan_glm(Concealing ~ 1, data=df) -->
<!-- fit_linear <- rstanarm::stan_glm(Concealing ~ Age, data=df) -->
<!-- fit_poly <- rstanarm::stan_glm(Concealing ~ poly(Age, 2, raw=TRUE), data=df) -->

<!-- # Compute the LOO cross-validation -->
<!-- loo_intercept_only <- rstanarm::loo(fit_intercept_only) -->
<!-- loo_linear <- rstanarm::loo(fit_linear) -->
<!-- loo_poly <- rstanarm::loo(fit_poly) -->

<!-- # Compare the models -->
<!-- comparison <- rstanarm::compare_models(loo_intercept_only, -->
<!--                                        loo_linear, -->
<!--                                        loo_poly) -->
<!-- print(comparison) -->
<!-- ``` -->
<!-- ```{r echo=FALSE, message=FALSE, warning=FALSE} -->
<!-- kable(comparison, digits=1) -->
<!-- ``` -->

<!-- As we can see, the best model (largest ELPD and smallest LOOIC) is the polynomial model, followed by the linear model. The worst model appears to be the constant (intercept-only) model. -->



## Piors Specification

One of the interesting aspect of the Bayesian framework is the possibility of adding prior expectations about the effect, to help model fitting and increase accuracy in noisy data or small samples.


### Weakly informative priors

As you might have notice, we didn't specify any priors in the previous analyses. In fact, we let the algorithm define and set *weakly informative priors*, designed to provide moderate regularization and help stabilize computation, without biasing the effect direction. For example, a wealky informative prior, for a standardized predictor (with mean = 0 and SD = 1) could be a normal distribution with mean = 0 and SD = 1. This means that the effect of this predictor is expected to be equally probable in any direction (as the distribution is symmetric around 0), with probability being higher close to 0 and lower far from 0.

While this prior doesn't bias the direction of the Bayesian (MCMC) sampling, it suggests that having an effect of 100 (*i.e.*, located at 100 SD of the mean as our variables are standardized) is highly unprobable, and that an effect close to 0 is more probable.

To better play with priors, let's start by standardizing our dataframe.


```{r, message=FALSE, results="hide"}
# Standardize (scale and center) the numeric variables
dfZ <- psycho::standardize(df)
```

Then, we can explicitly specify a weakly informative prior for all effects of the model.

```{r, message=FALSE, results="hide"}
# Let's fit our model
fit <- rstanarm::stan_glm(Life_Satisfaction ~ Tolerating, 
                          data=dfZ,
                          prior=normal(location = 0, # Mean
                                       scale = 1, # SD
                                       autoscale=FALSE)) # Don't adjust scale automatically
```

Let's plot the prior (the expectation) against the posterior (the estimated effect) distribution.

```{r, message=FALSE, results="hide"}
results <- psycho::analyze(fit)

# Extract the posterior
posterior <- results$values$effects$Tolerating$posterior

# Create a posterior with the prior and posterior distribution and plot them.
data.frame(posterior = posterior,
           prior = rnorm(length(posterior), 0, 1)) %>% 
  ggplot() +
  geom_density(aes(x=posterior), fill="lightblue", alpha=0.5) +
  geom_density(aes(x=prior), fill="blue", alpha=0.5) +
  scale_y_sqrt() # Change the Y axis so the plot is less ugly
```


This plot is rather ugly, because our posterior is very precise (due to the large sample) compared to the prior. 

### Informative priors

Although the default priors tend to work well, prudent use of more informative priors is encouraged. It is important to underline that setting informative priors (**if realistic**), does not overbias the analysis. In other words, is only "directs" the sampling: if the data are highly informative about the parameter values (enough to overwhelm the prior), a prudent informative prior (even if oppositive to the observed effect) will yield similar results to a non-informative prior. **In other words, you can't change the results dramatically by tweaking the priors**. But as the amount of data and/or the signal-to-noise ratio decrease, using a more informative prior becomes increasingly important. Of course, if you see someone using a prior with mean = 42 and SD = 0.0001, you should look at his results with caution...

Anyway, see the [official rstanarm documentation](https://CRAN.R-project.org/package=rstanarm/vignettes/priors.html) for details.








## Advanced Visualization

### Plot all iterations

As Bayesian models usually generate a lot of samples (*iterations*), one could want to plot them as well, instead (or along) the posterior "summary". This can be done quite easily by extracting all the iterations in `get_predicted`.

```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='hide', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
# Fit the model
fit <- rstanarm::stan_glm(Sex ~ Adjusting, data=df, family = "binomial")
```
```{r, fig.width=7, fig.height=4.5, eval = TRUE, results='markup', fig.align='center', comment=NA, message=FALSE, warning=FALSE}
# Generate a new refgrid
refgrid <- df %>% 
  select(Adjusting) %>% 
  psycho::refdata(length.out=10)

# Get predictions and keep iterations
predicted <- psycho::get_predicted(fit, newdata=refgrid, keep_iterations=TRUE)

# Reshape this dataframe to have iterations as factor
predicted <- predicted %>% 
  tidyr::gather(Iteration, Iteration_Value, starts_with("iter"))

# Plot iterations as well as the median prediction
ggplot(predicted, aes(x=Adjusting)) +
  geom_line(aes(y=Iteration_Value, group=Iteration), size=0.3, alpha=0.01) +
  geom_line(aes(y=Sex_Median), size=1) + 
  ylab("Male Probability\n")
```




## Credits

This package helped you? Don't forget to cite the various packages you used :)

You can cite `psycho` as follows:

- Makowski, (2018). *The psycho Package: an Efficient and Publishing-Oriented Workflow for Psychological Science*. Journal of Open Source Software, 3(22), 470. https://doi.org/10.21105/joss.00470

## Contribution

Improve this vignette by modifying [this](https://github.com/neuropsychology/psycho.R/blob/master/vignettes/bayesian.Rmd) file!
